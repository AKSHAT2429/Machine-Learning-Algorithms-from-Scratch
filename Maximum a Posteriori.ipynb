{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0264_Q9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS4KS-VQ2Xkh"
      },
      "source": [
        "IMPORTING IMPORTANT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-VsVFqfbGkD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import multi_dot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdin6sjs2Xnz"
      },
      "source": [
        "MOUNTING DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6JGHPgYbTb9",
        "outputId": "c9fda444-a074-4ea9-c365-429b26eb0941"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZCNN8ze2joI"
      },
      "source": [
        "DATA PRE-PROCESSING FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMRQfZvbbpWl"
      },
      "source": [
        "def func(train_output,train_features,test_output,test_features):\n",
        "  c0 = 0\n",
        "  c1 = 0\n",
        "  c2 = 0\n",
        "  mean0 = 0\n",
        "  mean1 = 0\n",
        "  mean2 = 0\n",
        "  for i in range(train_output.shape[0]):\n",
        "      if train_output[i] == 1:\n",
        "          c0 += 1\n",
        "          mean0 += train_features[i]\n",
        "\n",
        "      elif train_output[i] == 2:\n",
        "          c1 += 1\n",
        "          mean1 += train_features[i]\n",
        "      else:\n",
        "          c2 += 1\n",
        "          mean2 += train_features[i]\n",
        "  Py0 = c0 / train_output.shape[0]\n",
        "  Py1 = c1 / train_output.shape[0]\n",
        "  Py2 = c2 / train_output.shape[0]\n",
        "  mean0 = mean0 / c0\n",
        "  mean1 = mean1 / c1\n",
        "  mean2 = mean2 / c2\n",
        "  cov1 = np.cov((train_features - mean0).T)\n",
        "  cov2 = np.cov((train_features - mean1).T)\n",
        "  cov3 =  np.cov((train_features - mean2).T)\n",
        "  y = []\n",
        "  for i in range(test_features.shape[0]):\n",
        "      p1 = np.exp(-0.5 * multi_dot([(test_features[i] - mean0).T,\n",
        "                                    np.linalg.inv(cov1),\n",
        "                                    (test_features[i] - mean0)]))\n",
        "      cond_y1 = p1 / (np.power(2 * np.pi, 7 / 2) *\n",
        "                      np.power(np.linalg.det(cov1), 0.5))\n",
        "      \n",
        "      p2 = np.exp(-0.5 * multi_dot([(test_features[i] - mean1).T,\n",
        "                                    np.linalg.inv(cov2),\n",
        "                                    (test_features[i] - mean1)]))\n",
        "      \n",
        "      cond_y2 = p2 / (np.power(2 * np.pi, 7 / 2) *\n",
        "                      np.power(np.linalg.det(cov2), 0.5))\n",
        "      p3 = np.exp(-0.5 * multi_dot([(test_features[i] - mean2).T,\n",
        "                                    np.linalg.inv(cov3),\n",
        "                                    (test_features[i] - mean2)]))\n",
        "      \n",
        "      cond_y3 = p3 / (np.power(2 * np.pi, 7 / 2) *\n",
        "                      np.power(np.linalg.det(cov3), 0.5))\n",
        "      \n",
        "      P1 = cond_y1 * Py0 / (cond_y1 * Py0 + cond_y2 * Py1 + cond_y3 * Py2)\n",
        "      P2 = cond_y2 * Py1 / (cond_y1 * Py0 + cond_y2 * Py1 + cond_y3 * Py2)\n",
        "      P3 = cond_y3 * Py2 / (cond_y1 * Py0 + cond_y2 * Py1 + cond_y3 * Py2)\n",
        "      m = max(P1, P2, P3)\n",
        "      if m == P1:\n",
        "          y.append(1)\n",
        "      elif m == P2:\n",
        "          y.append(2)\n",
        "      else:\n",
        "          y.append(3)\n",
        "  cnt=0\n",
        "  for i in range(test_features.shape[0]):\n",
        "    if y[i]==test_output[i]:\n",
        "      cnt+=1\n",
        "  return print(\"Individual Accuracy :\",round(cnt*100/test_features.shape[0],4),'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNGoQiJx2nZf"
      },
      "source": [
        "MAXIMUM A POSTERIORI ALGORITHM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzpMC3FHnn_A"
      },
      "source": [
        "from random import Random\n",
        "dataset=pd.read_csv(\"/content/gdrive/MyDrive/NNFL_ASSIGNMENT_1/data_q6_q7.txt\",names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"output\"],delimiter=\"\\t\",error_bad_lines=False)\n",
        "Random(50).shuffle(dataset.values)\n",
        "dataset.reindex(np.random.permutation(dataset.index))\n",
        "data1 = dataset.dropna()\n",
        "data = data1.reset_index()\n",
        "\n",
        "X = data.drop('output',axis=1)\n",
        "y = data['output']\n",
        "\n",
        "def five_fold_batch_L2(X,y):\n",
        "  \n",
        "  m = len(y)\n",
        "  for i in range(5):\n",
        "    X_test = X[int(i*m*0.2):int((i+1)*m*0.2)]\n",
        "    X_train = pd.concat([X, X_test, X_test]).drop_duplicates(keep=False)\n",
        "    y_test = y[int(i*m*0.2):int((i+1)*m*0.2)]\n",
        "    y_train = y.drop(y_test.index)\n",
        "\n",
        "    train_features = np.asarray(X_train)\n",
        "    test_features = np.asarray(X_test)\n",
        "    train_output = np.asarray(y_train)\n",
        "    test_output = np.asarray(y_test)\n",
        "\n",
        "    x = []\n",
        "    mean = []\n",
        "    std = []\n",
        "    for i in range(train_features.shape[1]):\n",
        "        x.append((train_features[:, i] - np.mean(train_features[:, i])) /\n",
        "                np.std(train_features[:, i]))\n",
        "        mean.append(np.mean(train_features[:, i]))\n",
        "        std.append(np.std(train_features[:, i]))\n",
        "\n",
        "    train_features = np.array(x).T\n",
        "\n",
        "    x = []\n",
        "    for i in range(test_features.shape[1]):\n",
        "        x.append((test_features[:, i] - mean[i]) / std[i])\n",
        "\n",
        "    test_features = np.array(x).T\n",
        "\n",
        "    func(train_output,train_features,test_output,test_features)  \n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIn07lUF2-21"
      },
      "source": [
        "ACCURACY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP3CMa3lrnL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74f64a1-d19a-4e65-e865-b41ef35d0ac0"
      },
      "source": [
        "five_fold_batch_L2(X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual Accuracy : 100.0 %\n",
            "Individual Accuracy : 100.0 %\n",
            "Individual Accuracy : 97.5 %\n",
            "Individual Accuracy : 77.5 %\n",
            "Individual Accuracy : 92.5 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU9cyjoH3AkI"
      },
      "source": [
        "OVERALL ACCURACY : 93.5%"
      ]
    }
  ]
}